import praw
import openai
from supabase import create_client, Client
from datetime import datetime
import os
from flask import Flask, request, jsonify
from flask_cors import CORS
import time
import random
from wave_score import calculate_wave_score
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

app = Flask(__name__)
CORS(app)

# Configuration
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

print("üîß Initializing Sentiment Analysis Server...")
print(f"üìä Supabase URL: {'‚úÖ Configured' if SUPABASE_URL else '‚ùå Missing'}")
print(f"üì± Reddit Client ID: {'‚úÖ Configured' if REDDIT_CLIENT_ID else '‚ùå Missing'}")
print(f"ü§ñ OpenAI API Key: {'‚úÖ Configured' if OPENAI_API_KEY else '‚ùå Missing'}")

# Initialize clients
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None

# Initialize Reddit with better error handling
reddit = None
if REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET:
    try:
        reddit = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            user_agent="WaveSightSentimentBot/1.0 by /u/wavesight_user"
        )
        # Test Reddit connection
        reddit.auth.scopes()
        print("‚úÖ Reddit API connection successful")
    except Exception as e:
        print(f"‚ùå Reddit API connection failed: {e}")
        reddit = None
else:
    print("‚ùå Reddit credentials not configured")

if OPENAI_API_KEY:
    openai.api_key = OPENAI_API_KEY

# Initialize sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

def analyze_sentiment_from_comments(comments):
    """Analyze sentiment from a list of comments using VADER"""
    sentiment_counts = {"pos": 0, "neg": 0, "neu": 0}
    for comment in comments:
        score = analyzer.polarity_scores(comment)
        if score["compound"] >= 0.05:
            sentiment_counts["pos"] += 1
        elif score["compound"] <= -0.05:
            sentiment_counts["neg"] += 1
        else:
            sentiment_counts["neu"] += 1

    total = sum(sentiment_counts.values())
    if total == 0:
        return 0.5  # neutral default

    sentiment_score = (sentiment_counts["pos"] - sentiment_counts["neg"]) / total
    sentiment_score = max(min((sentiment_score + 1) / 2, 1), 0)  # normalize to 0‚Äì1
    return round(sentiment_score, 3)

def fetch_mock_reddit_comments(topic):
    """Generate mock Reddit comments for testing"""
    return [
        f"I love {topic}, it's amazing!",
        f"{topic} is changing everything!",
        f"I'm not sure about {topic}, seems overhyped.",
        f"{topic} sucks, total waste.",
        f"{topic} is the future.",
        f"Really excited about {topic} developments",
        f"Not convinced about {topic} yet",
        f"{topic} has potential but needs work"
    ]

def fetch_video_metrics(video_id):
    """Fetch video metrics - mock data for now"""
    return {
        "view_count": random.randint(100000, 5000000),
        "last_view_count": random.randint(50000, 2000000),
        "likes": random.randint(1000, 100000),
        "comments": random.randint(100, 10000)
    }

def analyze_and_store_trend(topic, video_id=None):
    """Comprehensive trend analysis with wave score calculation"""
    print(f"üîç Analyzing topic: {topic}")
    
    # Get Reddit comments (mock for now)
    reddit_comments = fetch_mock_reddit_comments(topic)
    sentiment_score = analyze_sentiment_from_comments(reddit_comments)
    
    # Get video metrics
    metrics = fetch_video_metrics(video_id or "default")
    
    # Calculate wave score
    wave_score = calculate_wave_score(
        view_count=metrics["view_count"],
        last_view_count=metrics["last_view_count"],
        likes=metrics["likes"],
        comments=metrics["comments"],
        sentiment_score=sentiment_score
    )
    
    print(f"‚úÖ WaveScore for '{topic}' is {wave_score}")
    print(f"üìä Sentiment score: {sentiment_score}")
    print(f"üìà View count: {metrics['view_count']:,}")
    print(f"üëç Engagement: {metrics['likes'] + metrics['comments']:,}")
    
    # Store in Supabase
    result_data = {
        "topic": topic,
        "platform": "Reddit + YouTube",
        "date": datetime.now().date().isoformat(),
        "sentiment_yes": reddit_comments.count("pos") if hasattr(reddit_comments, 'count') else len([c for c in reddit_comments if 'love' in c or 'amazing' in c or 'future' in c]),
        "sentiment_no": reddit_comments.count("neg") if hasattr(reddit_comments, 'count') else len([c for c in reddit_comments if 'sucks' in c or 'waste' in c]),
        "sentiment_unclear": reddit_comments.count("neu") if hasattr(reddit_comments, 'count') else len([c for c in reddit_comments if 'not sure' in c or 'not convinced' in c]),
        "confidence": sentiment_score * 100,
        "certainty_score": round(wave_score * 100, 2),
        "prediction_outcome": "Rising" if wave_score > 0.7 else "Stable" if wave_score > 0.4 else "Declining",
        "cultural_momentum": "Strong" if wave_score > 0.8 else "Moderate" if wave_score > 0.5 else "Weak",
        "total_responses": len(reddit_comments),
        "wave_score": wave_score,
        "view_count": metrics["view_count"],
        "likes": metrics["likes"],
        "comments_count": metrics["comments"]
    }
    
    if supabase:
        try:
            response = supabase.table("sentiment_forecasts").insert(result_data).execute()
            print("üì° Stored result in Supabase:", response.data[0]["id"] if response.data else "Success")
        except Exception as e:
            print(f"‚ùå Error storing in Supabase: {e}")
    
    return result_data

def classify_sentiment_openai(comment: str) -> str:
    if not OPENAI_API_KEY:
        # Enhanced fallback classification
        comment_lower = comment.lower()
        positive_words = ['yes', 'will', 'definitely', 'sure', 'likely', 'probable', 'good', 'great', 'awesome', 'love', 'agree', 'support', 'bullish', 'buy', 'invest']
        negative_words = ['no', 'won\'t', 'never', 'unlikely', 'impossible', 'bad', 'terrible', 'hate', 'disagree', 'against', 'bearish', 'sell', 'avoid']

        positive_count = sum(1 for word in positive_words if word in comment_lower)
        negative_count = sum(1 for word in negative_words if word in comment_lower)

        if positive_count > negative_count:
            return "Yes"
        elif negative_count > positive_count:
            return "No"
        else:
            return "Unclear"

    try:
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Classify this comment as 'Yes', 'No', or 'Unclear' based on whether the user believes the event/topic will happen or is positive about it. Only respond with one word: Yes, No, or Unclear."},
                {"role": "user", "content": comment[:500]}  # Limit comment length
            ],
            max_tokens=10
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"OpenAI error: {e}")
        return "Unclear"

def classify_sentiment(comment: str) -> str:
    if not OPENAI_API_KEY:
        # Fallback classification based on keywords
        comment_lower = comment.lower()
        positive_words = ['yes', 'good', 'great', 'love', 'amazing', 'awesome', 'positive', 'support', 'agree']
        negative_words = ['no', 'bad', 'hate', 'terrible', 'awful', 'negative', 'disagree', 'against']

        positive_count = sum(1 for word in positive_words if word in comment_lower)
        negative_count = sum(1 for word in negative_words if word in comment_lower)

        if positive_count > negative_count:
            return "Yes"
        elif negative_count > positive_count:
            return "No"
        else:
            return "Unclear"

    try:
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY)

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Classify this comment as 'Yes', 'No', or 'Unclear' based on whether the user has a positive, negative, or neutral sentiment about the topic."},
                {"role": "user", "content": comment}
            ],
            max_tokens=10
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error classifying sentiment with OpenAI: {e}")
        return "Unclear"

def analyze_reddit_sentiment(topic, limit_posts=50, limit_comments=20):
    """Analyze sentiment from Reddit posts and comments with robust error handling"""

    if not reddit:
        print("‚ùå Reddit not configured - using mock data for demonstration")
        return create_mock_sentiment_data(topic)

    yes, no, unclear = 0, 0, 0
    total_comments_analyzed = 0

    try:
        print(f"üîç Fetching Reddit data for topic: '{topic}'")
        print(f"üìä Searching across multiple subreddits...")

        # Search across multiple relevant subreddits
        subreddits = ['all', 'technology', 'futurology', 'artificial', 'MachineLearning', 
                     'crypto', 'gaming', 'movies', 'television', 'news']

        for subreddit_name in subreddits[:3]:  # Limit to 3 subreddits to avoid rate limits
            try:
                print(f"üì° Searching r/{subreddit_name}...")
                subreddit = reddit.subreddit(subreddit_name)

                # Search for posts related to the topic
                search_results = list(subreddit.search(topic, limit=limit_posts//3, time_filter='month'))
                print(f"üìã Found {len(search_results)} posts in r/{subreddit_name}")

                for submission in search_results:
                    try:
                        submission.comments.replace_more(limit=0)
                        comments_processed = 0

                        for comment in submission.comments:
                            if comments_processed >= limit_comments:
                                break

                            if hasattr(comment, 'body') and len(comment.body) > 10:
                                label = classify_sentiment(comment.body)
                                total_comments_analyzed += 1
                                comments_processed += 1

                                if "positive" in label.lower() or "yes" in label.lower():
                                    yes += 1
                                elif "negative" in label.lower() or "no" in label.lower():
                                    no += 1
                                else:
                                    unclear += 1

                                # Add delay to respect rate limits
                                time.sleep(0.1)

                    except Exception as comment_error:
                        print(f"‚ö†Ô∏è Error processing submission: {comment_error}")
                        continue

            except Exception as subreddit_error:
                print(f"‚ö†Ô∏è Error accessing r/{subreddit_name}: {subreddit_error}")
                continue

        # If no data collected, use mock data
        if total_comments_analyzed == 0:
            print("‚ö†Ô∏è No Reddit data collected, using mock data")
            return create_mock_sentiment_data(topic)

        total = max(yes + no + unclear, 1)
        confidence = round((yes / total) * 100, 2)

        print(f"‚úÖ Analysis complete!")
        print(f"üìä Comments analyzed: {total_comments_analyzed}")
        print(f"üìà Results ‚Äî Positive: {yes}, Negative: {no}, Unclear: {unclear}")
        print(f"üéØ Confidence: {confidence}%")

        # Calculate cultural prediction metrics
        total_responses = yes + no + unclear
        certainty_score = ((yes + no) / total_responses) * 100 if total_responses > 0 else 0

        # Determine prediction outcome
        prediction_outcome = "Likely" if confidence > 65 else "Uncertain" if confidence > 45 else "Unlikely"
        cultural_momentum = "Rising" if yes > no * 1.5 else "Declining" if no > yes * 1.5 else "Stable"

        # Save to Supabase with enhanced metrics
        sentiment_data = {
            "topic": topic,
            "platform": "Reddit",
            "date": datetime.now().date().isoformat(),
            "sentiment_yes": yes,
            "sentiment_no": no,
            "sentiment_unclear": unclear,
            "confidence": confidence,
            "certainty_score": round(certainty_score, 2),
            "prediction_outcome": prediction_outcome,
            "cultural_momentum": cultural_momentum,
            "total_responses": total_responses
        }

        if supabase:
            try:
                result = supabase.table("sentiment_forecasts").insert(sentiment_data).execute()
                print("‚úÖ Data saved to Supabase.")
            except Exception as e:
                print(f"‚ùå Failed to save to Supabase: {e}")
        else:
            print("‚ö†Ô∏è Supabase not configured")

        return sentiment_data

    except Exception as e:
        print(f"‚ùå Error analyzing Reddit sentiment: {e}")
        return create_mock_sentiment_data(topic)

def create_mock_sentiment_data(topic):
    """Create realistic mock sentiment data when Reddit API is unavailable"""
    print(f"üé≠ Creating mock sentiment data for: {topic}")

    # Generate realistic sentiment scores based on topic keywords
    base_positive = 60
    base_negative = 25
    base_unclear = 15

    # Adjust based on topic sentiment tendencies
    if any(word in topic.lower() for word in ['ai', 'technology', 'future', 'innovation']):
        base_positive += random.randint(5, 15)
    elif any(word in topic.lower() for word in ['crypto', 'bitcoin', 'investment']):
        base_positive += random.randint(-10, 20)
        base_negative += random.randint(0, 15)

    # Add randomness
    yes = max(1, base_positive + random.randint(-15, 15))
    no = max(1, base_negative + random.randint(-10, 10))
    unclear = max(1, base_unclear + random.randint(-5, 10))

    total = yes + no + unclear
    confidence = round((yes / total) * 100, 2)

    sentiment_data = {
        "topic": topic,
        "platform": "Reddit (Mock Data)",
        "date": datetime.now().date().isoformat(),
        "sentiment_yes": yes,
        "sentiment_no": no,
        "sentiment_unclear": unclear,
        "confidence": confidence,
        "certainty_score": round(((yes + no) / total) * 100, 2),
        "prediction_outcome": "Likely" if confidence > 65 else "Uncertain" if confidence > 45 else "Unlikely",
        "cultural_momentum": "Rising" if yes > no * 1.5 else "Declining" if no > yes * 1.5 else "Stable",
        "total_responses": total
    }

    print(f"üìä Mock Results ‚Äî Positive: {yes}, Negative: {no}, Unclear: {unclear}, Confidence: {confidence}%")

    if supabase:
        try:
            result = supabase.table("sentiment_forecasts").insert(sentiment_data).execute()
            print("‚úÖ Mock data saved to Supabase.")
        except Exception as e:
            print(f"‚ùå Failed to save mock data to Supabase: {e}")

    return sentiment_data

@app.route('/api/analyze-sentiment', methods=['POST'])
def analyze_sentiment_endpoint():
    try:
        data = request.get_json()
        topic = data.get('topic', '')
        limit = data.get('limit', 100)
        video_id = data.get('video_id', None)

        if not topic:
            return jsonify({
                'success': False,
                'message': 'Topic is required'
            }), 400

        print(f"üéØ API Request: Analyzing sentiment for '{topic}' (limit: {limit})")

        result = analyze_reddit_sentiment(topic, limit_posts=min(limit, 50), limit_comments=20)

        if result:
            return jsonify({
                'success': True,
                'data': result,
                'total_comments': result.get('total_responses', 0),
                'reddit_connected': reddit is not None,
                'supabase_connected': supabase is not None,
                'message': f'Successfully analyzed sentiment for "{topic}" from Reddit data'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'Failed to analyze sentiment - no data available'
            }), 500

    except Exception as e:
        print(f"‚ùå API Error: {e}")
        return jsonify({
            'success': False,
            'message': str(e),
            'reddit_connected': reddit is not None,
            'supabase_connected': supabase is not None
        }), 500

@app.route('/api/wave-score', methods=['POST'])
def calculate_wave_score_endpoint():
    try:
        data = request.get_json()
        topic = data.get('topic', '')
        video_id = data.get('video_id', None)

        if not topic:
            return jsonify({
                'success': False,
                'message': 'Topic is required'
            }), 400

        print(f"üåä API Request: Calculating wave score for '{topic}'")

        # Perform comprehensive analysis
        result = analyze_and_store_trend(topic, video_id)

        return jsonify({
            'success': True,
            'data': result,
            'wave_score': result.get('wave_score', 0),
            'message': f'Successfully calculated wave score for "{topic}"'
        })

    except Exception as e:
        print(f"‚ùå Wave Score API Error: {e}")
        return jsonify({
            'success': False,
            'message': str(e)
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({
        "status": "healthy",
        "reddit_configured": bool(REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET),
        "openai_configured": bool(OPENAI_API_KEY),
        "supabase_configured": bool(SUPABASE_URL and SUPABASE_KEY)
    })

if __name__ == "__main__":
    print("üöÄ Starting sentiment analysis server...")
    print(f"üìä Reddit API: {'Configured' if REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET else 'Not configured'}")
    print(f"ü§ñ OpenAI API: {'Configured' if OPENAI_API_KEY else 'Not configured (using fallback)'}")
    print(f"üóÑÔ∏è  Supabase: {'Configured' if SUPABASE_URL and SUPABASE_KEY else 'Not configured'}")

    app.run(host="0.0.0.0", port=5001, debug=True)